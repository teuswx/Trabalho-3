# Trabalho IA: Compara√ß√£o entre Algoritmos de Busca Informada

## Objetivo üéØ 

O trabalho tem como objetivo explorar e implementar √°rvores de decis√£o em diferentes contextos, com foco em dois aspectos principais:  

### 1. Desafio Interativo - √Årvore de Decis√£o com 10 Perguntas  
- Desenvolver uma √°rvore de decis√£o interativa capaz de ajudar os usu√°rios a escolher entre hobbies ou carreiras com base em suas prefer√™ncias e habilidades.  
- Estruturar a l√≥gica de perguntas e respostas para fornecer recomenda√ß√µes personalizadas de forma clara e intuitiva.  
- Estimular a reflex√£o pessoal ao explorar aspectos como trabalho em equipe, habilidades t√©cnicas, criatividade e comunica√ß√£o.  

### 2. Explora√ß√£o de Datasets Reais  
- Aplicar √°rvores de decis√£o para resolver problemas pr√°ticos utilizando datasets dispon√≠veis no Kaggle e no UCI Machine Learning Repository.  
- Analisar um dataset real, desde o pr√©-processamento at√© a cria√ß√£o de um modelo de √°rvore de decis√£o para classifica√ß√£o ou previs√£o.  
- Avaliar o desempenho do modelo com m√©tricas apropriadas, como acur√°cia, e apresentar os resultados de forma visual e documentada.  

### Dataset Escolhido para a parte 2 do trabalho üìà

Os dados escolhidos s√£o referentes ao Campeonato Mundial de F√≥rmula 1, abrangendo o per√≠odo de 1950 a 2024. O dataset foi obtido no Kaggle atrav√©s do seguinte link: [F√≥rmula 1 World Championship Dataset (1950-2020)](https://www.kaggle.com/datasets/rohanrao/formula-1-world-championship-1950-2020). Os dados foram filtrados em um arquivo para facilitar a an√°lise.

O objetivo √© analisar os dados de 2023 e 2024, utilizando os dados de 2023 para treinamento e os de 2024 para teste, tamb√©m prever quais pilotos mantiveram um desempenho consistente em 2024, com base no n√∫mero de voltas realizadas e na posi√ß√£o inicial no grid.  

#### Dados Selecionados  

Os dados utilizados para o treinamento incluem as seguintes colunas:  
- **`raceId`**: Identificador da corrida.  
- **`driverId`**: Identificador do piloto.  
- **`laps`**: N√∫mero de voltas completadas pelo piloto.  
- **`grid`**: Posi√ß√£o inicial no grid de largada.  

A vari√°vel alvo (**`target`**) √© uma coluna no arquivo `dadosFiltrados.xlsx`, que classifica os pilotos com base em sua pontua√ß√£o na corrida:  
- **1**: Pilotos com pontua√ß√£o acima de 10.  
- **0**: Pilotos com pontua√ß√£o abaixo de 10.  

Este crit√©rio foi estabelecido para identificar pilotos que apresentaram uma pontua√ß√£o consistente nas corridas, permitindo avaliar sua performance ao longo do campeonato. Como a coluna **`target`** utiliza como par√¢metro a pontua√ß√£o de cada piloto, m√©tricas muito relacionadas a pontua√ß√£o s√£o descartadas.

## Descri√ß√£o dos Algoritmos Implementados üìú  

- **Arquivo `arvore.py`**:  
  Implementa uma √°rvore de decis√£o iterativa com diversas perguntas para auxiliar o usu√°rio na escolha de hobbies ou carreiras com base em suas prefer√™ncias e respostas. O c√≥digo √© estruturado para fornecer sugest√µes personalizadas, criando uma intera√ß√£o simples.
  
- **Notebook `ML.ipynb`**:  
  Cont√©m o c√≥digo desenvolvido utilizando a biblioteca `scikit-learn` para an√°lise e previs√£o com base no dataset selecionado. O notebook realiza o pr√©-processamento dos dados, treina uma √°rvore de decis√£o para identificar padr√µes e faz previs√µes sobre o desempenho dos pilotos no dataset de F√≥rmula 1.

## Algoritmos de Classifica√ß√£o üìä

Al√©m da √°rvore de decis√£o, foram implementados outros algoritmos de classifica√ß√£o para comparar seu desempenho. Os algoritmos escolhidos s√£o:

### 1. **Support Vector Machine (SVM)**  
SVM √© um algoritmo de aprendizado supervisionado que pode ser usado tanto para problemas de classifica√ß√£o quanto de regress√£o. O principal objetivo do SVM √© encontrar um hiperplano que melhor separe as diferentes classes em um espa√ßo de caracter√≠sticas. No contexto de classifica√ß√£o, o SVM tenta encontrar a "margem m√°xima" entre as classes, o que significa que ele busca o hiperplano que tem a maior dist√¢ncia de qualquer ponto de dados de ambas as classes.

- **Vantagens:**
  - Eficiente em espa√ßos de alta dimens√£o.
  - Funciona bem em casos onde o n√∫mero de dimens√µes √© maior do que o n√∫mero de amostras.
  - Utiliza o conceito de margem, o que pode resultar em uma boa generaliza√ß√£o.

- **Desvantagens:**
  - Requer mais tempo de treinamento em grandes datasets.
  - Dif√≠cil de interpretar em alguns casos.
  - Sens√≠vel a outliers.

### 2. **K-Nearest Neighbors (KNN)**  
O KNN √© um algoritmo de aprendizado supervisionado que classifica um ponto de dados com base na classe da maioria de seus vizinhos mais pr√≥ximos no espa√ßo das caracter√≠sticas. A ideia √© simples: para classificar uma amostra, o algoritmo encontra os K vizinhos mais pr√≥ximos e faz a classifica√ß√£o com base na maioria das classes desses vizinhos.

- **Vantagens:**
  - F√°cil de entender e implementar.
  - N√£o requer um modelo expl√≠cito de treinamento, o que torna o treinamento r√°pido.
  - Funciona bem com dados n√£o-lineares e com muitas classes.

- **Desvantagens:**
  - Pode ser computacionalmente caro, especialmente com grandes volumes de dados.
  - Sens√≠vel ao valor de K e √† escolha da dist√¢ncia.
  - Pode sofrer de alta varia√ß√£o em datasets com ru√≠do.

## Estrutura do Projeto üèóÔ∏è

- **Linguagem:** Python.
- **Bibliotecas Utilizadas:**
  - **pandas**: Para manipula√ß√£o e an√°lise de dados, incluindo carregamento e pr√©-processamento do dataset.
  - **scikit-learn**:
    - `DecisionTreeClassifier`: Para cria√ß√£o e treinamento da √°rvore de decis√£o.
    - `SVC`: Para cria√ß√£o e treinamento do modelo SVM.
    - `KNeighborsClassifier`: Para cria√ß√£o e treinamento do modelo KNN.
    - `train_test_split`: Para dividir os dados em conjuntos de treino e teste.
    - `accuracy_score`: Para calcular a acur√°cia do modelo.
    - `plot_tree`: Para visualiza√ß√£o gr√°fica da √°rvore de decis√£o.
  - **matplotlib.pyplot**: Para criar gr√°ficos e visualizar a √°rvore de decis√£o.

## Resultados das Medi√ß√µes de Desempenho ‚è±Ô∏è

### Decision Tree
Com o modelo desenvolvido, foi poss√≠vel analisar os dados do dataset e visualizar a √°rvore de decis√£o gerada durante o treinamento. A √°rvore de decis√£o foi constru√≠da para classificar os pilotos com base nos atributos selecionados, e sua estrutura fornece uma vis√£o clara das regras usadas pelo modelo para fazer as previs√µes.  

Abaixo est√° a visualiza√ß√£o da √°rvore de decis√£o treinada:

![√Årvore de Decis√£o](https://github.com/user-attachments/assets/c73e75f1-aa2e-4b4e-9434-749bd2bc3342)

Foi poss√≠vel obter uma acur√°cia de **0.88**, o que significa um valor **muito bom** de acerto para previs√£o.

# KNN (K-Nearest Neighbors)

K-Nearest Neighbors (KNN) √© um algoritmo de aprendizado supervisionado usado para **classifica√ß√£o** e **regress√£o**. Ele baseia suas previs√µes na proximidade dos dados de entrada em rela√ß√£o a outros pontos do conjunto de treinamento.

## Funcionamento do KNN
1. **Escolha do n√∫mero de vizinhos (K)**: Define quantos vizinhos mais pr√≥ximos ser√£o considerados na predi√ß√£o.
2. **C√°lculo da dist√¢ncia entre os pontos**: Normalmente, usa-se a **dist√¢ncia Euclidiana** para medir a proximidade.
3. **Sele√ß√£o dos K vizinhos mais pr√≥ximos**.
4. **Classifica√ß√£o ou regress√£o**:
   - **Classifica√ß√£o**: O novo ponto recebe a classe mais frequente entre os vizinhos.
   - **Regress√£o**: O novo ponto recebe a m√©dia dos valores dos vizinhos.

---

## C√°lculo da Dist√¢ncia no KNN

O KNN usa f√≥rmulas matem√°ticas para medir a similaridade entre os pontos. A mais comum √© a **dist√¢ncia Euclidiana**, definida por:

$$
d(p, q) = \sqrt{\sum_{i=1}^{n} (q_i - p_i)^2}
$$

Onde:
- \( p \) e \( q \) s√£o dois pontos no espa√ßo n-dimensional.
- \( q_i \) e \( p_i \) s√£o as coordenadas dos pontos nos diferentes atributos.
- \( d(p, q) \) √© a dist√¢ncia Euclidiana entre os pontos.

### Exemplo de C√°lculo:
Se temos dois pontos \( A(2, 3) \) e \( B(5, 7) \), a dist√¢ncia Euclidiana entre eles √©:

$$
d(A, B) = \sqrt{(5 - 2)^2 + (7 - 3)^2}
$$

$$
d(A, B) = \sqrt{3^2 + 4^2} = \sqrt{9 + 16} = \sqrt{25} = 5
$$

Al√©m da dist√¢ncia Euclidiana, outras m√©tricas podem ser usadas, como:
- **Dist√¢ncia de Manhattan**: 

$$
d(p, q) = \sum |q_i - p_i|
$$

- **Dist√¢ncia de Minkowski**: Generaliza as anteriores com um par√¢metro \( p \):

$$
d(p, q) = \left( \sum |q_i - p_i|^p \right)^{\frac{1}{p}}
$$

---

## Vari√¢ncia doa acur√°cia do KNN
A acur√°cia do KNN pode variar por diferentes motivos, mesmo usando os mesmos dados:

- **Escolha do valor de K**: Valores diferentes podem gerar resultados distintos.
- **Divis√£o dos dados (Treino/Teste)**: Pequenas varia√ß√µes na separa√ß√£o dos dados podem mudar a classifica√ß√£o.
- **Normaliza√ß√£o dos dados**: A escala dos atributos pode influenciar no c√°lculo das dist√¢ncias.
- **Pontos de fronteira**: Pequenas altera√ß√µes nos dados podem mudar os vizinhos mais pr√≥ximos, impactando a predi√ß√£o.

Ap√≥s o embaralhamento dos dados, a m√©dia passou a oscilar entre 0,8577 e 0,8661. Embora a diferen√ßa entre esses valores seja insignificativa, observou-se uma altera√ß√£o na vari√¢ncia dos dados ap√≥s o processo de embaralhamento. Isso indica que o embaralhamento teve um impacto relevante na dispers√£o dos valores, mesmo que a m√©dia tenha se mantido em uma faixa estreita.
 
### Support Vector Machine (SVM)

Support Vector Machine (SVM) √© um algoritmo de aprendizado de m√°quina supervisionado usado principalmente para classifica√ß√£o, mas tamb√©m pode ser aplicado em regress√£o. Ele trabalha encontrando o hiperplano √≥timo que separa as classes de forma mais eficiente poss√≠vel.

---

## Como Funciona o SVM?

1. **Margem M√°xima**:
   - O SVM busca encontrar o hiperplano que maximiza a margem entre as classes de dados. A margem √© definida como a dist√¢ncia entre o hiperplano e os pontos mais pr√≥ximos de cada classe, chamados de **vetores de suporte**.

2. **Vetores de Suporte**:
   - S√£o os pontos de dados que est√£o mais pr√≥ximos do hiperplano e definem sua posi√ß√£o. A remo√ß√£o de qualquer um desses pontos pode alterar a posi√ß√£o do hiperplano.

3. **Hiperplano √ìtimo**:
   - O hiperplano √© uma linha (em 2D), um plano (em 3D) ou um hiperplano em dimens√µes superiores que separa as classes da melhor maneira poss√≠vel.

4. **Classifica√ß√£o Linear e N√£o Linear**:
   - Se os dados forem linearmente separ√°veis, o SVM encontrar√° um hiperplano reto.
   - Se os dados n√£o forem linearmente separ√°veis, usa-se o **truque do kernel**, que transforma os dados para um espa√ßo dimensional superior onde se tornam linearmente separ√°veis.

---

## Fun√ß√£o do Kernel

O kernel √© uma fun√ß√£o que transforma os dados em um espa√ßo dimensional superior para tornar a separa√ß√£o poss√≠vel. Os principais tipos de kernel s√£o:

- **Linear**: $ K(x, y) = x \cdot y $
- **Polinomial**: $ K(x, y) = (x \cdot y + c)^d $
- **RBF (Radial Basis Function)**: $ K(x, y) = e^{-\gamma ||x - y||^2} $
- **Sigm√≥ide**: $ K(x, y) = \tanh(x \cdot y + c) $

---

## F√≥rmula Matem√°tica do SVM

Dado um conjunto de treinamento com $ n $ amostras $ (x_i, y_i) $, onde $ y_i \in \{-1, 1\} $, o SVM resolve o seguinte problema de otimiza√ß√£o:

$$
\min_{{w, b}} \frac{1}{2} ||w||^2 \quad \text{sujeito a } \quad y_i (w \cdot x_i + b) \geq 1, \forall i
$$

Caso haja erros de classifica√ß√£o, usa-se a vers√£o com **vari√°veis de folga** $ \xi_i $:

$$
\min_{{w, b, \xi}} \frac{1}{2} ||w||^2 + C \sum \xi_i \quad \text{sujeito a } \quad y_i (w \cdot x_i + b) \geq 1 - \xi_i, \forall i
$$

Onde $ C $ √© um hiperpar√¢metro que controla o trade-off entre margem e erro de classifica√ß√£o.

---

## Aplica√ß√µes do SVM

- **Reconhecimento de padr√µes** (exemplo: reconhecimento de escrita)
- **Diagn√≥stico m√©dico** (exemplo: detec√ß√£o de c√¢ncer)
- **Detec√ß√£o de fraudes**
- **Processamento de imagens**

---

## Vantagens e Desvantagens

### Vantagens:
- Funciona bem em conjuntos de dados com margens claras de separa√ß√£o.
- Eficiente em espa√ßos de alta dimens√£o.
- Usa um subconjunto de pontos de treinamento (vetores de suporte), tornando-o eficiente em termos de mem√≥ria.

### Desvantagens:
- Sens√≠vel √† escolha do tipo de kernel e dos hiperpar√¢metros.
- Pode ser computacionalmente caro para conjuntos de dados muito grandes.

---

## Conclus√£o

O SVM √© um poderoso algoritmo de aprendizado de m√°quina usado principalmente para classifica√ß√£o. Seu princ√≠pio central √© encontrar um hiperplano √≥timo que maximize a margem entre as classes. Com o uso de fun√ß√µes de kernel, ele pode lidar com dados n√£o linearmente separ√°veis, tornando-se uma ferramenta vers√°til para diversas aplica√ß√µes.

Com o SVM foi poss√≠vel obter uma acur√°cia de 0.86, o que √© um valor muito bom de acertos
## Pilotos com Maior Chance de Obter Pontos em 2025

Com base nas previs√µes realizadas, foi poss√≠vel obter as probabilidades m√©dias de cada piloto para conseguir pontos acima de 10. Abaixo est√£o os **10 pilotos com maior chance** de alcan√ßar esse feito, classificados em ordem decrescente de probabilidade:

| Posi√ß√£o | Driver ID | Forename    | Surname    | Probabilidade de Pontos |
|---------|-----------|-------------|------------|-------------------------|
| 1       | 830       | Max         | Verstappen | 1.0000                  |
| 2       | 846       | Lando       | Norris     | 0.8333                  |
| 3       | 844       | Charles     | Leclerc    | 0.7500                  |
| 4       | 832       | Carlos      | Sainz      | 0.7273                  |
| 5       | 815       | Sergio      | P√©rez      | 0.6667                  |
| 6       | 857       | Oscar       | Piastri    | 0.6667                  |
| 7       | 847       | George      | Russell    | 0.5833                  |
| 8       | 1         | Lewis       | Hamilton   | 0.5000                  |
| 9       | 4         | Fernando    | Alonso     | 0.3333                  |
| 10      | 807       | Nico        | H√ºlkenberg | 0.0833                  |

### An√°lise dos Resultados

- **Max Verstappen** lidera com uma probabilidade de **1.0000**, indicando que ele √© o piloto com a maior chance de conseguir pontos em 2025.
- **Lando Norris** e **Charles Leclerc** tamb√©m t√™m chances muito altas, com probabilidades de **0.8333** e **0.7500**, respectivamente.
- Outros pilotos como **Carlos Sainz** e **Sergio P√©rez** tamb√©m apresentam boas chances, com probabilidades superiores a 0.66.
- **Lewis Hamilton** e **Fernando Alonso** est√£o em posi√ß√µes mais baixas, mas ainda t√™m chances razo√°veis de alcan√ßar pontos.
- **Nico H√ºlkenberg** apresenta a menor probabilidade entre os 10 primeiros, com **0.0833**, sugerindo uma chance consideravelmente baixa.

Esses resultados s√£o baseados em um modelo de previs√£o que utiliza dados hist√≥ricos para estimar as probabilidades de cada piloto alcan√ßar mais de 10 pontos ao longo da temporada de 2025.

## Considera√ß√µes Finais üìù

O trabalho foi desenvolvido com o objetivo de explorar a aplica√ß√£o de √°rvores de decis√£o em diferentes contextos, com √™nfase na previs√£o de desempenho de pilotos de F√≥rmula 1 para a temporada de 2025. Atrav√©s do uso de dados hist√≥ricos, foi poss√≠vel criar um modelo que permitiu calcular a probabilidade de cada piloto alcan√ßar pontos acima de 10, com uma acur√°cia de **0.88**.

O trabalho foi realizado em duas partes principais: a cria√ß√£o de um modelo interativo para ajudar usu√°rios a escolher entre hobbies ou carreiras para entender melhor como funciona uma √°rvore de decis√£o, e a utiliza√ß√£o de um dataset real de F√≥rmula 1 para prever a performance dos pilotos em 2025. O modelo mostrou um bom desempenho, refletido na sua acur√°cia e nas previs√µes de probabilidade de sucesso de cada piloto.

Os resultados obtidos, principalmente para pilotos como **Max Verstappen** e **Lando Norris**, confirmam a consist√™ncia do modelo e sua capacidade de identificar padr√µes de performance em dados hist√≥ricos. Em termos de impacto, a an√°lise pode ser √∫til para ver pilotos que possuem melhor const√¢ncia e desempenho, oferecendo uma base s√≥lida para previs√µes e decis√µes baseadas em dados. 

Por fim, √© importante ressaltar que, apesar da acur√°cia obtida, sempre h√° espa√ßo para melhorias no modelo, como a inclus√£o de mais vari√°veis que possam influenciar o desempenho dos pilotos, o que pode aprimorar ainda mais as previs√µes em cen√°rios futuros.

## Compila√ß√£o e Execu√ß√£o 

A pilha din√¢mica disponibilizada possui um arquivo Makefile que realiza todo o procedimento de compila√ß√£o e execu√ß√£o. Para tanto, temos as seguintes diretrizes de execu√ß√£o:

| Comando                |      Fun√ß√£o                                                                                           |                     
| -----------------------| ------------------------------------------------------------------------------------------------- |
|  `python analisef1.py` | Realiza a filtragem dos dados necess√°rios para an√°lise                                            |
|  `python arvore.py`    |              Roda o c√≥digo da parte 1 do trabalho                                                 |

## Autores

- Frank Leite Lemos Costa ‚Äì Aluno do 6¬∫ per√≠odo de Engenharia da Computa√ß√£o.
- Mateus Henrique Pereira ‚Äì Aluno do 8¬∫ per√≠odo de Engenharia da Computa√ß√£o.
